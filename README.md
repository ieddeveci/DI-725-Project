# Efficient Fine-Tuning of PaliGemma for Image Captioning


This repository contains the final project for the **Transformers and Attention-Based Deep Networks** course. 

The project focuses on enhancing the image captioning capabilities of **PaliGemma**, a vision-language model (VLM), through fine-tuning with the **RISC dataset**.

The primary goal is to improve the baseline performance of PaliGemma in generating image captions by:

- Applying **efficient fine-tuning strategies**, including parameter-efficient fine-tuning (PEFT) techniques.
- Performing **hyperparameter optimization** to identify effective training configurations.
- Integrating **evaluation metrics** that capture both linguistic fluency and visual-semantic alignment.
