{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df44bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from google.colab import drive, userdata\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03febdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/\n",
    "\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "model_name = \"google/paligemma-3b-pt-224\"\n",
    "lora_checkpoint = \"./qlora_output/checkpoint-7500\"\n",
    "dataset_dir = 'RISCM'\n",
    "images_dir = os.path.join(dataset_dir, 'resized')\n",
    "captions_path = os.path.join(dataset_dir, 'captions_cleaned.csv')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eecc5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, lora_checkpoint, is_trainable=False)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "processor = PaliGemmaProcessor.from_pretrained(model_name, token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942c44f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running single inference\n",
    "\n",
    "df = pd.read_csv(captions_path, usecols=['image', 'split', 'training_caption'])\n",
    "df.columns = df.columns.str.strip()\n",
    "test_df = df[df[\"split\"].str.lower() == \"test\"].reset_index(drop=True)\n",
    "\n",
    "sample_row = test_df.iloc[56]\n",
    "image_file = sample_row['image'].strip()\n",
    "image_path = os.path.join(images_dir, image_file)\n",
    "prompt = \"<image> <bos> Describe this image in detail:\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_caption(image):\n",
    "    inputs = processor(\n",
    "        text=prompt,\n",
    "        images=image,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "        do_convert_rgb=True\n",
    "    ).to(device, dtype=model.dtype)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=16,\n",
    "        num_beams=1,\n",
    "        do_sample=False\n",
    "    #    num_beams=4,\n",
    "    #    do_sample=True,\n",
    "    #    top_p=0.9,\n",
    "    #    top_k=50,\n",
    "    #    temperature=0.8,\n",
    "    #    early_stopping=True,\n",
    "    #    length_penalty=1.0,\n",
    "    #    no_repeat_ngram_size=2\n",
    "    )\n",
    "\n",
    "    caption = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    return caption.replace(\"Describe this image in detail:\", \"\").strip()\n",
    "\n",
    "generated_caption = generate_caption(image)\n",
    "print(\"Image file:\", image_file)\n",
    "print(\"Generated Caption:\", generated_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74ceb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running inference on the test set\n",
    "\n",
    "batch_size = 32\n",
    "fraction = 1.0\n",
    "output_csv = os.path.join(dataset_dir, 'predictions.csv')\n",
    "\n",
    "df = pd.read_csv(captions_path, usecols=['image', 'split', 'training_caption'])\n",
    "df.columns = df.columns.str.strip()\n",
    "test_df = df[df[\"split\"].str.lower() == \"test\"].sample(frac=fraction, random_state=42).reset_index(drop=True)\n",
    "image_names = test_df['image'].tolist()\n",
    "\n",
    "def load_single_image(fname, images_dir):\n",
    "    img_path = os.path.join(images_dir, fname.strip())\n",
    "    with Image.open(img_path) as img:\n",
    "        return fname, img.convert(\"RGB\")\n",
    "\n",
    "def load_images_parallel(batch_filenames, num_workers=4):\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        load_fn = partial(load_single_image, images_dir=images_dir)\n",
    "        results = list(executor.map(load_fn, batch_filenames))\n",
    "\n",
    "    valid_pairs = [(f, img) for f, img in results if img is not None]\n",
    "    if valid_pairs:\n",
    "        return tuple(zip(*valid_pairs))\n",
    "    else:\n",
    "        return [], []\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_captions(images):\n",
    "    input_texts = [\"<image> <bos> Describe this image in detail:\" for _ in images]\n",
    "\n",
    "    inputs = processor(\n",
    "        text=input_texts,\n",
    "        images=images,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "        do_convert_rgb=True\n",
    "    ).to(device, dtype=model.dtype)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=16,\n",
    "        num_beams=1,\n",
    "        do_sample=False\n",
    "    #    num_beams=4,\n",
    "    #    do_sample=True,\n",
    "    #    top_p=0.9,\n",
    "    #    top_k=50,\n",
    "    #    temperature=0.8,\n",
    "    #    early_stopping=True,\n",
    "    #    length_penalty=1.0,\n",
    "    #    no_repeat_ngram_size=2\n",
    "    )\n",
    "\n",
    "    captions = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return [caption.replace(\"Describe this image in detail:\", \"\").strip() for caption in captions]\n",
    "\n",
    "generated_captions = [\"\"] * len(image_names)\n",
    "\n",
    "for i in tqdm(range(0, len(image_names), batch_size), desc=\"Processing images\"):\n",
    "    batch_files = image_names[i:i+batch_size]\n",
    "    batch_filenames, batch_imgs = load_images_parallel(batch_files)\n",
    "\n",
    "    if len(batch_imgs) == 0:\n",
    "        continue\n",
    "\n",
    "    preds = generate_captions(list(batch_imgs))\n",
    "\n",
    "    pred_dict = dict(zip(batch_filenames, preds))\n",
    "    for j, fname in enumerate(batch_files):\n",
    "        if fname in pred_dict:\n",
    "            generated_captions[i + j] = pred_dict[fname]\n",
    "\n",
    "test_df[\"predicted_caption\"] = generated_captions\n",
    "\n",
    "test_df.to_csv(output_csv, index=False)\n",
    "print(f\"\\nPredictions saved to: {output_csv}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
