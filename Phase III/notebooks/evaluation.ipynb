{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073feb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), 'CLIP'))\n",
    "import clip\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b624033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoother = SmoothingFunction().method4\n",
    "\n",
    "def compute_bleu_scores(references, hypothesis):\n",
    "    references_tok = [ref.split() for ref in references]\n",
    "    hypothesis_tok = hypothesis.split()\n",
    "    bleu1 = sentence_bleu(references_tok, hypothesis_tok, weights=(1, 0, 0, 0), smoothing_function=smoother)\n",
    "    bleu2 = sentence_bleu(references_tok, hypothesis_tok, weights=(0.5, 0.5, 0, 0), smoothing_function=smoother)\n",
    "    return bleu1, bleu2\n",
    "\n",
    "def compute_meteor_score_nltk(references, hypothesis):\n",
    "    references_tok = [ref.split() for ref in references]\n",
    "    hypothesis_tok = hypothesis.split()\n",
    "    return min(1.0, meteor_score(references_tok, hypothesis_tok))\n",
    "\n",
    "def evaluate(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    bleu1_scores = []\n",
    "    bleu2_scores = []\n",
    "    meteor_scores = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Evaluating {os.path.basename(filepath)}\"):\n",
    "        references = [str(row['training_caption']).lower()]\n",
    "        hypothesis = str(row['predicted_caption']).lower()\n",
    "\n",
    "        bleu1, bleu2 = compute_bleu_scores(references, hypothesis)\n",
    "        meteor = compute_meteor_score_nltk(references, hypothesis)\n",
    "\n",
    "        bleu1_scores.append(bleu1)\n",
    "        bleu2_scores.append(bleu2)\n",
    "        meteor_scores.append(meteor)\n",
    "\n",
    "    df['bleu1_pred'] = bleu1_scores\n",
    "    df['bleu2_pred'] = bleu2_scores\n",
    "    df['meteor_pred'] = meteor_scores\n",
    "\n",
    "    output_file = filepath.replace(\".csv\", \"_with_scores.csv\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"\\n{os.path.basename(output_file)}\")\n",
    "    print(f\"BLEU-1:   {np.mean(bleu1_scores):.4f}\")\n",
    "    print(f\"BLEU-2:   {np.mean(bleu2_scores):.4f}\")\n",
    "    print(f\"METEOR:   {np.mean(meteor_scores):.4f}\")\n",
    "\n",
    "for file in os.listdir(\".\"):\n",
    "    if file.endswith(\".csv\") and not file.endswith(\"_with_scores.csv\"):\n",
    "        evaluate(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dba602",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "model = model.to(device)\n",
    "\n",
    "image_folder = \"RISCM/resized/\"\n",
    "\n",
    "def compute_clip_score(image, caption):\n",
    "    image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "    text_input = clip.tokenize([caption]).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input)\n",
    "        text_features = model.encode_text(text_input)\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = (image_features @ text_features.T)\n",
    "        return similarity.item()\n",
    "\n",
    "def normalize_score(s, min_val=0.05, max_val=0.40):\n",
    "    if s is None:\n",
    "        return None\n",
    "    norm = (s - min_val) / (max_val - min_val)\n",
    "    return max(0.0, min(1.0, norm))\n",
    "\n",
    "def evaluate_clip(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    clip_scores = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Scoring {os.path.basename(filepath)}\"):\n",
    "        image_path = os.path.join(image_folder, row[\"image\"])\n",
    "\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to open image at {image_path}: {e}\")\n",
    "            clip_scores.append(None)\n",
    "            continue\n",
    "\n",
    "        caption = str(row[\"predicted_caption\"])\n",
    "        score = compute_clip_score(image, caption)\n",
    "        clip_scores.append(score)\n",
    "\n",
    "    df[\"clip_score\"] = clip_scores\n",
    "\n",
    "    valid_scores = [s for s in clip_scores if s is not None]\n",
    "    if not valid_scores:\n",
    "        print(f\"No valid CLIP scores for {filepath}\")\n",
    "        return\n",
    "\n",
    "    min_score = 0.05\n",
    "    max_score = 0.40\n",
    "\n",
    "    normalized_scores = [normalize_score(s, min_score, max_score) for s in clip_scores]\n",
    "    df[\"clip_score_normalized\"] = normalized_scores\n",
    "\n",
    "    output_file = filepath.replace(\".csv\", \"_with_clip_scores.csv\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    overall_clip_avg = sum(valid_scores) / len(valid_scores)\n",
    "    overall_norm_avg = sum([s for s in normalized_scores if s is not None]) / len(valid_scores)\n",
    "\n",
    "    print(f\"\\n{os.path.basename(output_file)}\")\n",
    "    print(f\"  CLIP Avg:         {overall_clip_avg:.4f}\")\n",
    "    print(f\"  Normalized Avg:   {overall_norm_avg:.4f}\")\n",
    "    print(f\"  Normalization bounds: Min = {min_score:.4f}, Max = {max_score:.4f}\")\n",
    "\n",
    "for file in os.listdir(\".\"):\n",
    "    if file.endswith(\".csv\") and not file.endswith(\"_with_clip_scores.csv\"):\n",
    "        evaluate_clip(file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
